# Copyright (c) 2025 XBXyftx
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import json
import logging
import threading
import time
from typing import List, Optional, Dict, Any
from datetime import datetime
from enum import Enum

from models.news import NewsArticle, NewsResponse
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from services.openharmony_image_crawler import OpenHarmonyImageCrawler

logger = logging.getLogger(__name__)

class ServiceStatus(str, Enum):
    """æœåŠ¡çŠ¶æ€æšä¸¾"""
    READY = "ready"           # æœåŠ¡å°±ç»ª
    PREPARING = "preparing"   # å‡†å¤‡ä¸­ï¼ˆæ•°æ®æ›´æ–°ä¸­ï¼‰
    ERROR = "error"           # é”™è¯¯çŠ¶æ€

class NewsCache:
    """æ–°é—»æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self._cache: List[NewsArticle] = []
        self._cache_lock = threading.RLock()  # å¯é‡å…¥é”
        self._status = ServiceStatus.READY  # åˆå§‹çŠ¶æ€ä¸ºå°±ç»ªï¼Œç­‰å¾…æ•°æ®åˆ†æ‰¹å†™å…¥
        self._last_update = None
        self._update_count = 0
        self._error_message = None
        self._is_updating = False  # æ ‡è®°æ˜¯å¦æ­£åœ¨æ›´æ–°
        self._is_first_load = True  # æ ‡è®°æ˜¯å¦ä¸ºé¦–æ¬¡åŠ è½½
        
    def get_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        with self._cache_lock:
            return {
                "status": self._status.value,
                "last_update": self._last_update,
                "cache_count": len(self._cache),
                "update_count": self._update_count,
                "error_message": self._error_message,
                "is_updating": self._is_updating,
                "is_first_load": self._is_first_load  # æ·»åŠ é¦–æ¬¡åŠ è½½æ ‡è¯†
            }
    
    def set_status(self, status: ServiceStatus, error_message: Optional[str] = None):
        """è®¾ç½®æœåŠ¡çŠ¶æ€"""
        with self._cache_lock:
            self._status = status
            self._error_message = error_message
            logger.info(f"æœåŠ¡çŠ¶æ€æ›´æ–°: {status.value}")
    
    def set_updating(self, is_updating: bool):
        """è®¾ç½®æ›´æ–°çŠ¶æ€"""
        with self._cache_lock:
            self._is_updating = is_updating
            if is_updating:
                logger.info("å¼€å§‹æ•°æ®æ›´æ–°ï¼ŒçŠ¶æ€è®¾ä¸ºå‡†å¤‡ä¸­")
                self.set_status(ServiceStatus.PREPARING)
            else:
                logger.info("æ•°æ®æ›´æ–°å®Œæˆï¼ŒçŠ¶æ€è®¾ä¸ºå°±ç»ª")
                self.set_status(ServiceStatus.READY)
    
    def get_news(self, page: int = 1, page_size: int = 20, 
                 category: Optional[str] = None, 
                 search: Optional[str] = None) -> NewsResponse:
        """è·å–æ–°é—»æ•°æ®ï¼ˆå¸¦åˆ†é¡µå’Œè¿‡æ»¤ï¼‰"""
        with self._cache_lock:
            if self._status == ServiceStatus.ERROR:
                raise Exception(f"æœåŠ¡é”™è¯¯: {self._error_message}")
            
            # è¿‡æ»¤æ•°æ®
            filtered_news = self._cache.copy()
            
            # åˆ†ç±»è¿‡æ»¤
            if category:
                filtered_news = [news for news in filtered_news if news.category == category]
            
            # æœç´¢è¿‡æ»¤
            if search:
                search_lower = search.lower()
                filtered_news = [
                    news for news in filtered_news 
                    if search_lower in news.title.lower() or 
                       (news.summary and search_lower in news.summary.lower())
                ]
            
            # ğŸ”¥ æ”¹è¿›ï¼šç”±äºç¼“å­˜å†™å…¥æ—¶å·²ç»æ’åºï¼Œè¿™é‡Œåªåšè½»é‡çº§éªŒè¯
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ’åºï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
            try:
                if len(filtered_news) > 1:
                    # æ£€æŸ¥å‰ä¸¤ç¯‡æ–‡ç« çš„æ—¥æœŸé¡ºåº
                    first_date = self._parse_date_for_sorting(filtered_news[0].date)
                    second_date = self._parse_date_for_sorting(filtered_news[1].date)
                    
                    # å¦‚æœé¡ºåºä¸å¯¹ï¼Œé‡æ–°æ’åº
                    if first_date < second_date:
                        logger.info("ğŸ”„ [è¯»å–æ’åº] æ£€æµ‹åˆ°é¡ºåºå¼‚å¸¸ï¼Œæ‰§è¡Œé‡æ–°æ’åº")
                        filtered_news.sort(key=lambda x: self._parse_date_for_sorting(x.date), reverse=True)
                    else:
                        logger.debug("âœ… [è¯»å–æ’åº] æ—¥æœŸé¡ºåºæ­£ç¡®ï¼Œæ— éœ€é‡æ–°æ’åº")
            except Exception as e:
                logger.warning(f"âš ï¸ [è¯»å–æ’åº] æ—¥æœŸé¡ºåºæ£€æŸ¥å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é¡ºåº: {e}")
            
            # åˆ†é¡µå¤„ç†
            total = len(filtered_news)
            start = (page - 1) * page_size
            end = start + page_size
            paginated_news = filtered_news[start:end]
            
            return NewsResponse(
                articles=paginated_news,
                total=total,
                page=page,
                page_size=page_size,
                has_next=end < total,
                has_prev=page > 1
            )
    
    def _parse_date_for_sorting(self, date_str: str) -> datetime:
        """
        è§£ææ—¥æœŸå­—ç¬¦ä¸²ç”¨äºæ’åºï¼Œæ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼
        æ”¯æŒæ ¼å¼ï¼š2024-08-31, 2024.08.31, 2024/08/31, 2024å¹´08æœˆ31æ—¥ç­‰
        """
        try:
            if not date_str or not isinstance(date_str, str):
                logger.warning(f"æ— æ•ˆçš„æ—¥æœŸå­—ç¬¦ä¸²: {date_str}")
                return datetime(1970, 1, 1)
            
            # æ¸…ç†æ—¥æœŸå­—ç¬¦ä¸²
            date_str = date_str.strip()
            
            # 1. ä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ—¥æœŸç»„ä»¶ï¼Œæ”¯æŒæ›´å¤šæ ¼å¼
            import re
            
            # åŒ¹é… YYYY-MM-DD, YYYY.MM.DD, YYYY/MM/DD æ ¼å¼ï¼ˆæ”¯æŒå•æ•°å­—æœˆ/æ—¥ï¼‰
            date_patterns = [
                r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})',  # 2024-08-31, 2024.08.31, 2024/08/31
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥?',      # 2024å¹´08æœˆ31æ—¥
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})',        # 2024å¹´08æœˆ31
                r'(\d{1,2})[-./](\d{1,2})[-./](\d{4})',  # 31-08-2024, 31.08.2024 (DD-MM-YYYY)
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, date_str)
                if match:
                    groups = match.groups()
                    try:
                        if len(groups) == 3:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯DD-MM-YYYYæ ¼å¼
                            if pattern.startswith(r'(\d{1,2})'):
                                day, month, year = groups
                            else:
                                year, month, day = groups
                            
                            year = int(year)
                            month = int(month)
                            day = int(day)
                            
                            # éªŒè¯æ—¥æœŸæœ‰æ•ˆæ€§
                            if 1 <= month <= 12 and 1 <= day <= 31 and 1900 <= year <= 2100:
                                parsed_date = datetime(year, month, day)
                                logger.debug(f"âœ… æ—¥æœŸè§£ææˆåŠŸ: {date_str} -> {parsed_date.strftime('%Y-%m-%d')}")
                                return parsed_date
                    except (ValueError, TypeError) as e:
                        logger.debug(f"æ­£åˆ™åŒ¹é…ä½†è½¬æ¢å¤±è´¥: {date_str}, é”™è¯¯: {e}")
                        continue
            
            # 2. å¦‚æœæ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ ‡å‡†datetimeæ ¼å¼
            date_formats = [
                '%Y-%m-%d',           # 2024-08-31
                '%Y.%m.%d',           # 2024.08.31
                '%Y/%m/%d',           # 2024/08/31
                '%Y-%m-%d %H:%M:%S',  # 2024-08-31 10:30:00
                '%Y.%m.%d %H:%M:%S',  # 2024.08.31 10:30:00
                '%Y/%m/%d %H:%M:%S',  # 2024/08/31 10:30:00
                '%Yå¹´%mæœˆ%dæ—¥',        # 2024å¹´08æœˆ31æ—¥
                '%Yå¹´%mæœˆ%d',          # 2024å¹´08æœˆ31
                '%m-%d',              # 08-31 (å‡è®¾ä¸ºå½“å¹´)
                '%m.%d',              # 08.31 (å‡è®¾ä¸ºå½“å¹´)
                '%m/%d',              # 08/31 (å‡è®¾ä¸ºå½“å¹´)
            ]
            
            for date_format in date_formats:
                try:
                    parsed_date = datetime.strptime(date_str, date_format)
                    # å¦‚æœåªæœ‰æœˆæ—¥ï¼Œè¡¥å……å½“å‰å¹´ä»½
                    if '%Y' not in date_format:
                        parsed_date = parsed_date.replace(year=datetime.now().year)
                    logger.debug(f"âœ… strptimeè§£ææˆåŠŸ: {date_str} -> {parsed_date.strftime('%Y-%m-%d')}")
                    return parsed_date
                except ValueError:
                    continue
            
            # 3. æœ€åçš„å°è¯•ï¼šæå–ä»»ä½•å¯èƒ½çš„æ•°å­—ç»„åˆ
            number_match = re.findall(r'\d+', date_str)
            if len(number_match) >= 3:
                try:
                    # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯å¹´ä»½ï¼ˆå¦‚æœæ˜¯4ä½æ•°ï¼‰ï¼Œå¦åˆ™æŒ‰é¡ºåºå¤„ç†
                    nums = [int(x) for x in number_match[:3]]
                    
                    # åˆ¤æ–­å¹´ä»½ä½ç½®
                    if nums[0] > 1900:  # ç¬¬ä¸€ä¸ªæ•°å­—æ˜¯å¹´ä»½
                        year, month, day = nums[0], nums[1], nums[2]
                    elif nums[2] > 1900:  # ç¬¬ä¸‰ä¸ªæ•°å­—æ˜¯å¹´ä»½
                        day, month, year = nums[0], nums[1], nums[2]
                    else:
                        # é»˜è®¤å½“å‰å¹´ä»½
                        year = datetime.now().year
                        month, day = nums[0], nums[1]
                    
                    if 1 <= month <= 12 and 1 <= day <= 31:
                        parsed_date = datetime(year, month, day)
                        logger.debug(f"âœ… æ•°å­—æå–è§£ææˆåŠŸ: {date_str} -> {parsed_date.strftime('%Y-%m-%d')}")
                        return parsed_date
                except (ValueError, IndexError):
                    pass
            
            # æœ€åçš„fallbackï¼šè¿”å›Unixçºªå…ƒæ—¶é—´
            logger.warning(f"âš ï¸ æ— æ³•è§£ææ—¥æœŸæ ¼å¼: '{date_str}'ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´")
            return datetime(1970, 1, 1)
            
        except Exception as e:
            logger.error(f"âŒ æ—¥æœŸè§£æå¼‚å¸¸: '{date_str}', é”™è¯¯: {e}")
            return datetime(1970, 1, 1)
    
    def _sort_articles_by_date(self, articles: List[NewsArticle]) -> List[NewsArticle]:
        """
        æŒ‰æ—¥æœŸå¯¹æ–‡ç« è¿›è¡Œæ’åºï¼ˆç”±è¿‘åˆ°è¿œï¼‰
        åœ¨æ•°æ®åˆå¹¶æ—¶ç»Ÿä¸€è§¦å‘æ’åºï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        """
        try:
            if not articles:
                return articles
            
            logger.info(f"ğŸ”„ [ç¼“å­˜æ’åº] å¼€å§‹å¯¹ {len(articles)} ç¯‡æ–‡ç« è¿›è¡Œæ—¥æœŸæ’åº...")
            
            # ç»Ÿè®¡æ—¥æœŸè§£ææƒ…å†µ
            parse_stats = {"success": 0, "failed": 0, "examples": []}
            
            def sort_key_with_stats(article):
                parsed_date = self._parse_date_for_sorting(article.date)
                if parsed_date.year > 1970:
                    parse_stats["success"] += 1
                    if len(parse_stats["examples"]) < 3:
                        parse_stats["examples"].append(f"{article.date} -> {parsed_date.strftime('%Y-%m-%d')}")
                else:
                    parse_stats["failed"] += 1
                return parsed_date
            
            # æ‰§è¡Œæ’åº
            sorted_articles = sorted(articles, key=sort_key_with_stats, reverse=True)
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            total = len(articles)
            success_rate = (parse_stats["success"] / total * 100) if total > 0 else 0
            
            logger.info(f"âœ… [ç¼“å­˜æ’åº] æ’åºå®Œæˆï¼")
            logger.info(f"ğŸ“Š [ç¼“å­˜æ’åº] æˆåŠŸè§£æ: {parse_stats['success']}/{total} ({success_rate:.1f}%)")
            if parse_stats["failed"] > 0:
                logger.warning(f"âš ï¸ [ç¼“å­˜æ’åº] è§£æå¤±è´¥: {parse_stats['failed']} ç¯‡æ–‡ç« ")
            
            if parse_stats["examples"]:
                logger.info(f"ğŸ“… [ç¼“å­˜æ’åº] è§£æç¤ºä¾‹: {', '.join(parse_stats['examples'])}")
            
            # æ˜¾ç¤ºæ’åºåçš„å‰å‡ ç¯‡æ–‡ç« çš„æ—¥æœŸ
            if sorted_articles:
                latest_dates = [article.date for article in sorted_articles[:3]]
                logger.info(f"ğŸ“ˆ [ç¼“å­˜æ’åº] æœ€æ–°æ–‡ç« æ—¥æœŸ: {latest_dates}")
            
            return sorted_articles
            
        except Exception as e:
            logger.error(f"âŒ [ç¼“å­˜æ’åº] æ’åºå¤±è´¥: {e}")
            return articles  # æ’åºå¤±è´¥æ—¶è¿”å›åŸåˆ—è¡¨
    
    def update_cache(self, news_data: List[NewsArticle]):
        """æ›´æ–°ç¼“å­˜æ•°æ®ï¼ˆå®Œå…¨æ›¿æ¢ï¼‰"""
        with self._cache_lock:
            try:
                # è®¾ç½®æ›´æ–°çŠ¶æ€ä¸ºTrueï¼ŒçŠ¶æ€å˜ä¸ºå‡†å¤‡ä¸­
                self.set_updating(True)
                
                # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šåœ¨æ•°æ®åˆå¹¶æ—¶è§¦å‘æ—¥æœŸæ’åº
                logger.info(f"ğŸ”„ [å®Œæ•´æ›´æ–°] å¼€å§‹æ›´æ–°ç¼“å­˜ï¼ŒåŸå§‹æ•°æ®: {len(news_data)} ç¯‡æ–‡ç« ")
                sorted_news_data = self._sort_articles_by_date(news_data.copy())
                
                # æ›´æ–°ç¼“å­˜
                self._cache = sorted_news_data
                self._last_update = datetime.now().isoformat()
                self._update_count += 1
                
                # ğŸ”¥ é‡è¦ï¼šæ ‡è®°é¦–æ¬¡åŠ è½½å®Œæˆï¼Œåç»­ä¸å†ä½¿ç”¨åˆ†æ‰¹å†™å…¥
                if self._is_first_load:
                    self._is_first_load = False
                    logger.info("ğŸ é¦–æ¬¡å®Œæ•´åŠ è½½å®Œæˆï¼Œåç»­æ›´æ–°å°†ä½¿ç”¨å®Œæ•´æ›¿æ¢æ¨¡å¼")
                
                # è®¾ç½®æ›´æ–°çŠ¶æ€ä¸ºFalseï¼ŒçŠ¶æ€å˜ä¸ºå°±ç»ª
                self.set_updating(False)
                
                logger.info(f"ğŸ”„ ç¼“å­˜å®Œæ•´æ›´æ–°æˆåŠŸï¼Œå…± {len(news_data)} æ¡æ–°é—»")
                
            except Exception as e:
                error_msg = f"ç¼“å­˜æ›´æ–°å¤±è´¥: {str(e)}"
                self.set_status(ServiceStatus.ERROR, error_msg)
                self._is_updating = False
                logger.error(error_msg)
                raise
    
    def append_to_cache(self, new_articles: List[NewsArticle]):
        """å¢é‡è¿½åŠ æ–°æ–‡ç« åˆ°ç¼“å­˜ï¼ˆä»…ç”¨äºé¦–æ¬¡åŠ è½½çš„åˆ†æ‰¹å†™å…¥ï¼‰"""
        with self._cache_lock:
            try:
                if not new_articles:
                    return
                
                # ğŸ”¥ é‡è¦ï¼šåªåœ¨é¦–æ¬¡åŠ è½½æ—¶æ‰å…è®¸åˆ†æ‰¹å†™å…¥
                if not self._is_first_load:
                    logger.warning("âš ï¸ éé¦–æ¬¡åŠ è½½ï¼Œå¿½ç•¥åˆ†æ‰¹å†™å…¥ï¼Œç­‰å¾…å®Œæ•´æ›´æ–°")
                    return
                
                # è·å–ç°æœ‰æ–‡ç« çš„URLé›†åˆï¼Œç”¨äºå»é‡
                existing_urls = {article.url for article in self._cache}
                
                # è¿‡æ»¤æ‰é‡å¤çš„æ–‡ç« 
                unique_articles = [
                    article for article in new_articles 
                    if article.url not in existing_urls
                ]
                
                if unique_articles:
                    # è¿½åŠ æ–°æ–‡ç« 
                    self._cache.extend(unique_articles)
                    
                    # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šåˆ†æ‰¹å†™å…¥åç«‹å³è§¦å‘æ’åºï¼Œä¿æŒæ•°æ®ä¸€è‡´æ€§
                    logger.info(f"ğŸ”„ [åˆ†æ‰¹æ›´æ–°] è¿½åŠ  {len(unique_articles)} ç¯‡æ–‡ç« åè§¦å‘æ’åº")
                    self._cache = self._sort_articles_by_date(self._cache)
                    
                    self._last_update = datetime.now().isoformat()
                    
                    # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šå¦‚æœç¼“å­˜ä¸­æœ‰æ–‡ç« äº†ï¼Œå°±è®¾ç½®çŠ¶æ€ä¸ºREADY
                    if len(self._cache) > 0 and self._status == ServiceStatus.PREPARING:
                        logger.info("ğŸ‰ ç¼“å­˜ä¸­å·²æœ‰æ•°æ®ï¼ŒçŠ¶æ€è®¾ä¸ºå°±ç»ªï¼Œç”¨æˆ·å¯ä»¥å¼€å§‹æŸ¥çœ‹æ–‡ç« ")
                        self.set_status(ServiceStatus.READY)
                    
                    logger.info(f"ğŸ“ [é¦–æ¬¡åŠ è½½] å¢é‡è¿½åŠ  {len(unique_articles)} ç¯‡æ–°æ–‡ç« åˆ°ç¼“å­˜ï¼ˆè·³è¿‡ {len(new_articles) - len(unique_articles)} ç¯‡é‡å¤ï¼‰")
                    logger.info(f"ğŸ“Š [é¦–æ¬¡åŠ è½½] ç¼“å­˜æ€»æ•°: {len(self._cache)} ç¯‡æ–‡ç« ")
                else:
                    logger.info(f"ğŸ“ [é¦–æ¬¡åŠ è½½] æœ¬æ‰¹æ¬¡ {len(new_articles)} ç¯‡æ–‡ç« å…¨éƒ¨ä¸ºé‡å¤ï¼Œè·³è¿‡")
                
            except Exception as e:
                error_msg = f"å¢é‡æ›´æ–°ç¼“å­˜å¤±è´¥: {str(e)}"
                logger.error(error_msg)
                raise
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        with self._cache_lock:
            return {
                "cache_size": len(self._cache),
                "last_update": self._last_update,
                "update_count": self._update_count,
                "status": self._status.value,
                "error_message": self._error_message,
                "is_updating": self._is_updating
            }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._cache_lock:
            self._cache.clear()
            self._last_update = None
            self._update_count = 0
            self.set_updating(True)  # æ¸…ç©ºæ—¶è®¾ä¸ºå‡†å¤‡ä¸­
            logger.info("ç¼“å­˜å·²æ¸…ç©º")

# å…¨å±€ç¼“å­˜å®ä¾‹
_news_cache: Optional[NewsCache] = None

def get_news_cache() -> NewsCache:
    """è·å–æ–°é—»ç¼“å­˜å®ä¾‹"""
    global _news_cache
    if _news_cache is None:
        _news_cache = NewsCache()
    return _news_cache

def init_cache():
    """åˆå§‹åŒ–ç¼“å­˜"""
    global _news_cache, _banner_cache
    _news_cache = NewsCache()
    _banner_cache = BannerCache()
    logger.info("æ–°é—»ç¼“å­˜åˆå§‹åŒ–å®Œæˆ")
    logger.info("è½®æ’­å›¾ç¼“å­˜åˆå§‹åŒ–å®Œæˆ")


class BannerCache:
    """è½®æ’­å›¾ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self._cache: List[Dict[str, Any]] = []
        self._cache_lock = threading.RLock()
        self._status = ServiceStatus.PREPARING  # åˆå§‹çŠ¶æ€ä¸ºå‡†å¤‡ä¸­ï¼Œç­‰å¾…é¦–æ¬¡çˆ¬å–
        self._last_update = None
        self._update_count = 0
        self._error_message = None
        self._is_updating = False
        self._first_load_completed = False  # æ ‡è®°æ˜¯å¦å®Œæˆé¦–æ¬¡åŠ è½½
        
    def get_status(self) -> Dict[str, Any]:
        """è·å–è½®æ’­å›¾æœåŠ¡çŠ¶æ€"""
        with self._cache_lock:
            return {
                "status": self._status.value,
                "last_update": self._last_update,
                "cache_count": len(self._cache),
                "update_count": self._update_count,
                "error_message": self._error_message,
                "is_updating": self._is_updating,
                "first_load_completed": self._first_load_completed
            }
    
    def set_status(self, status: ServiceStatus, error_message: Optional[str] = None):
        """è®¾ç½®æœåŠ¡çŠ¶æ€"""
        with self._cache_lock:
            self._status = status
            self._error_message = error_message
            logger.info(f"è½®æ’­å›¾æœåŠ¡çŠ¶æ€æ›´æ–°: {status.value}")
    
    def set_updating(self, is_updating: bool):
        """è®¾ç½®æ›´æ–°çŠ¶æ€"""
        with self._cache_lock:
            self._is_updating = is_updating
            if is_updating:
                logger.info("å¼€å§‹è½®æ’­å›¾æ•°æ®æ›´æ–°ï¼ŒçŠ¶æ€è®¾ä¸ºå‡†å¤‡ä¸­")
                self.set_status(ServiceStatus.PREPARING)
            else:
                logger.info("è½®æ’­å›¾æ•°æ®æ›´æ–°å®Œæˆï¼ŒçŠ¶æ€è®¾ä¸ºå°±ç»ª")
                self.set_status(ServiceStatus.READY)
    
    def get_banner_images(self) -> List[Dict[str, Any]]:
        """è·å–è½®æ’­å›¾æ•°æ®"""
        with self._cache_lock:
            if self._status == ServiceStatus.ERROR:
                raise Exception(f"è½®æ’­å›¾æœåŠ¡é”™è¯¯: {self._error_message}")
            
            return self._cache.copy()
    
    def update_cache(self, banner_data: List[Dict[str, Any]]):
        """æ›´æ–°è½®æ’­å›¾ç¼“å­˜æ•°æ®"""
        with self._cache_lock:
            try:
                # è®¾ç½®æ›´æ–°çŠ¶æ€
                self.set_updating(True)
                
                # æ›´æ–°ç¼“å­˜
                self._cache = banner_data.copy()
                self._last_update = datetime.now().isoformat()
                self._update_count += 1
                
                # æ ‡è®°é¦–æ¬¡åŠ è½½å®Œæˆ
                if not self._first_load_completed:
                    self._first_load_completed = True
                    logger.info("ğŸ‰ è½®æ’­å›¾é¦–æ¬¡åŠ è½½å®Œæˆ")
                
                # è®¾ç½®å®ŒæˆçŠ¶æ€ - åªæœ‰åœ¨æœ‰æ•°æ®æ—¶æ‰è®¾ä¸ºREADY
                if len(banner_data) > 0:
                    self.set_updating(False)  # è¿™ä¼šè®¾ç½®ä¸ºREADY
                    logger.info(f"ğŸ–¼ï¸ è½®æ’­å›¾ç¼“å­˜æ›´æ–°æˆåŠŸï¼Œå…± {len(banner_data)} å¼ å›¾ç‰‡ï¼ŒçŠ¶æ€ï¼šREADY")
                else:
                    # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œä¿æŒPREPARINGçŠ¶æ€
                    self._is_updating = False
                    self._status = ServiceStatus.PREPARING
                    logger.warning("âš ï¸ è½®æ’­å›¾ç¼“å­˜æ›´æ–°å®Œæˆï¼Œä½†æœªè·å–åˆ°æ•°æ®ï¼ŒçŠ¶æ€ä¿æŒï¼šPREPARING")
                
            except Exception as e:
                error_msg = f"è½®æ’­å›¾ç¼“å­˜æ›´æ–°å¤±è´¥: {str(e)}"
                self.set_status(ServiceStatus.ERROR, error_msg)
                self._is_updating = False
                logger.error(error_msg)
                raise
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–è½®æ’­å›¾ç¼“å­˜ä¿¡æ¯"""
        with self._cache_lock:
            return {
                "cache_size": len(self._cache),
                "last_update": self._last_update,
                "update_count": self._update_count,
                "status": self._status.value,
                "error_message": self._error_message,
                "is_updating": self._is_updating
            }
    
    def clear_cache(self):
        """æ¸…ç©ºè½®æ’­å›¾ç¼“å­˜"""
        with self._cache_lock:
            self._cache.clear()
            self._last_update = None
            self._update_count = 0
            self.set_updating(True)
            logger.info("è½®æ’­å›¾ç¼“å­˜å·²æ¸…ç©º")


# å…¨å±€è½®æ’­å›¾ç¼“å­˜å®ä¾‹
_banner_cache: Optional[BannerCache] = None

def get_banner_cache() -> BannerCache:
    """è·å–è½®æ’­å›¾ç¼“å­˜å®ä¾‹"""
    global _banner_cache
    if _banner_cache is None:
        _banner_cache = BannerCache()
    return _banner_cache 