# Copyright (c) 2025 XBXyftx
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from typing import Optional
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor

from .cache import get_news_cache, get_banner_cache, ServiceStatus
from services.news_service import get_news_service, NewsSource

logger = logging.getLogger(__name__)

class TaskScheduler:
    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        self.thread_pool = ThreadPoolExecutor(max_workers=6, thread_name_prefix="CrawlerWorker")
        self._setup_jobs()
    
    def _setup_jobs(self):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        try:
            # æ¯6å°æ—¶æ›´æ–°ä¸€æ¬¡æ‰€æœ‰æ–°é—»æºï¼ˆé™ä½é¢‘ç‡ä»¥å‡å°‘èµ„æºä¸æµé‡ï¼‰
            self.scheduler.add_job(
                lambda: self._update_cache_job(NewsSource.ALL),
                trigger=IntervalTrigger(hours=6),
                id='update_cache_all',
                name='æ›´æ–°æ‰€æœ‰æ–°é—»æºç¼“å­˜',
                replace_existing=True
            )
            
            # æ¯6å°æ—¶æ›´æ–°ä¸€æ¬¡è½®æ’­å›¾ï¼ˆä¸æ–°é—»æ›´æ–°æ—¶é—´é—´éš”ä¸€è‡´ï¼‰
            self.scheduler.add_job(
                self._update_banner_cache_job,
                trigger=IntervalTrigger(hours=6),
                id='update_banner_cache',
                name='æ›´æ–°è½®æ’­å›¾ç¼“å­˜',
                replace_existing=True
            )
            
            # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œå®Œæ•´çˆ¬å–ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
            self.scheduler.add_job(
                self._full_crawl_job,
                trigger=CronTrigger(hour=2, minute=0),
                id='full_crawl',
                name='å®Œæ•´çˆ¬å–ä»»åŠ¡',
                replace_existing=True
            )
            
            logger.info("å®šæ—¶ä»»åŠ¡è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"è®¾ç½®å®šæ—¶ä»»åŠ¡å¤±è´¥: {e}")
    
    def _run_crawler_in_thread(self, task_name: str, source: NewsSource = NewsSource.ALL):
        """åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œçˆ¬è™«ä»»åŠ¡"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œ{task_name} - æ¥æº: {source.value}")
            
            # è·å–ç¼“å­˜å®ä¾‹å’Œæ–°é—»æœåŠ¡
            cache = get_news_cache()
            news_service = get_news_service()
            
            logger.info(f"ğŸ“Š {task_name} - å‡†å¤‡å¹¶è¡Œçˆ¬å–æ•°æ®...")
            
            # æ‰§è¡Œçˆ¬å–ï¼ˆåˆ†æ‰¹å†™å…¥æ¨¡å¼ï¼Œæ•°æ®å·²ç»åœ¨çˆ¬å–è¿‡ç¨‹ä¸­å†™å…¥ç¼“å­˜ï¼‰
            articles = news_service.crawl_news(source)
            
            logger.info(f"ğŸ” {task_name} - çˆ¬å–å®Œæˆï¼ŒåŸå§‹æ–‡ç« æ•°: {len(articles)}")
            
            # éªŒè¯æ–‡ç« æ•°æ®
            valid_articles = news_service.validate_articles(articles)
            
            logger.info(f"âœ… {task_name} - éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆæ–‡ç« æ•°: {len(valid_articles)}")
            
            # ğŸ”¥ é‡è¦ï¼šæ ¹æ®æ˜¯å¦é¦–æ¬¡åŠ è½½å†³å®šæ›´æ–°ç­–ç•¥
            if cache._is_first_load:
                # é¦–æ¬¡åŠ è½½ï¼šæ•°æ®å·²ç»é€šè¿‡åˆ†æ‰¹å†™å…¥ï¼Œåªéœ€ç¡®ä¿çŠ¶æ€æ­£ç¡®
                cache_status = cache.get_status()
                if cache_status['cache_count'] > 0 and cache_status['status'] != 'ready':
                    logger.info(f"ğŸ¯ {task_name} - ç¡®ä¿ç¼“å­˜çŠ¶æ€ä¸ºå°±ç»ª")
                    cache.set_status(ServiceStatus.READY)
                logger.info(f"ğŸ‰ {task_name}å®Œæˆï¼ˆé¦–æ¬¡åŠ è½½ï¼‰ï¼Œç¼“å­˜ä¸­å…±æœ‰ {cache_status['cache_count']} ç¯‡æ–‡ç« ")
            else:
                # åç»­æ›´æ–°ï¼šå®Œæ•´æ›¿æ¢ç¼“å­˜ï¼Œé¿å…æ•°æ®å€’é€€
                logger.info(f"ğŸ”„ {task_name} - æ‰§è¡Œå®Œæ•´ç¼“å­˜æ›´æ–°ï¼ˆéé¦–æ¬¡åŠ è½½ï¼‰")
                cache.update_cache(valid_articles)
                cache_status = cache.get_status()
                logger.info(f"ğŸ‰ {task_name}å®Œæˆï¼ˆå®Œæ•´æ›´æ–°ï¼‰ï¼Œç¼“å­˜ä¸­å…±æœ‰ {cache_status['cache_count']} ç¯‡æ–‡ç« ")
            
        except Exception as e:
            logger.error(f"âŒ {task_name}å¤±è´¥: {e}", exc_info=True)
            # è®¾ç½®é”™è¯¯çŠ¶æ€
            cache = get_news_cache()
            cache.set_status(ServiceStatus.ERROR, str(e))
    
    async def _update_cache_job(self, source: NewsSource = NewsSource.ALL):
        """å®šæ—¶æ›´æ–°ç¼“å­˜ä»»åŠ¡"""
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
            task_name = f"å®šæ—¶ç¼“å­˜æ›´æ–°ä»»åŠ¡ - {source.value}"
            future = self.thread_pool.submit(self._run_crawler_in_thread, task_name, source)
            # ä¸ç­‰å¾…å®Œæˆï¼Œè®©ä»»åŠ¡åœ¨åå°æ‰§è¡Œ
            logger.info(f"{task_name}å·²æäº¤åˆ°åå°çº¿ç¨‹")
            
        except Exception as e:
            logger.error(f"æäº¤å®šæ—¶ç¼“å­˜æ›´æ–°ä»»åŠ¡å¤±è´¥: {e}")
    
    async def _full_crawl_job(self):
        """å®Œæ•´çˆ¬å–ä»»åŠ¡ï¼ˆæ‰€æœ‰æ¥æºï¼‰"""
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
            future = self.thread_pool.submit(self._run_crawler_in_thread, "å®Œæ•´çˆ¬å–ä»»åŠ¡", NewsSource.ALL)
            # ä¸ç­‰å¾…å®Œæˆï¼Œè®©ä»»åŠ¡åœ¨åå°æ‰§è¡Œ
            logger.info("å®Œæ•´çˆ¬å–ä»»åŠ¡å·²æäº¤åˆ°åå°çº¿ç¨‹")
            
        except Exception as e:
            logger.error(f"æäº¤å®Œæ•´çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}")
    
    def _run_banner_crawler_in_thread(self, task_name: str):
        """åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œè½®æ’­å›¾çˆ¬è™«ä»»åŠ¡"""
        try:
            logger.info(f"ğŸ–¼ï¸ å¼€å§‹æ‰§è¡Œ{task_name}")
            
            # è·å–è½®æ’­å›¾ç¼“å­˜å®ä¾‹
            banner_cache = get_banner_cache()
            
            # è®¾ç½®ä¸ºæ­£åœ¨æ›´æ–°çŠ¶æ€ï¼ˆè¿™ä¼šå°†çŠ¶æ€è®¾ä¸ºPREPARINGï¼‰
            banner_cache.set_updating(True)
            
            # ä¼˜å…ˆä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°ä¼ ç»Ÿçˆ¬è™«
            banner_info_list = []
            
            import os
            use_enhanced = os.getenv("BANNER_USE_ENHANCED", "true").lower() == "true"
            try:
                if use_enhanced:
                    # å°è¯•ä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«
                    from services.enhanced_mobile_banner_crawler import EnhancedMobileBannerCrawler
                    enhanced_crawler = EnhancedMobileBannerCrawler()
                    banner_info_list = enhanced_crawler.crawl_mobile_banners(
                        download_images=False,  # å®šæ—¶ä»»åŠ¡ä¸ä¸‹è½½å›¾ç‰‡
                        save_directory=""
                    )
                    logger.info(f"âœ… ä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«æˆåŠŸï¼Œè·å– {len(banner_info_list)} å¼ å›¾ç‰‡")
                else:
                    logger.info("â­ï¸ å·²é€šè¿‡ç¯å¢ƒå˜é‡ç¦ç”¨å¢å¼ºç‰ˆçˆ¬è™«ï¼Œç›´æ¥ä½¿ç”¨ä¼ ç»Ÿçˆ¬è™«")
                    raise Exception("Enhanced crawler disabled by env")
            except Exception as enhanced_error:
                logger.warning(f"âš ï¸ å¢å¼ºç‰ˆçˆ¬è™«ä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œå°è¯•ä¼ ç»Ÿçˆ¬è™«: {enhanced_error}")
                # å›é€€åˆ°ä¼ ç»Ÿçˆ¬è™«ï¼ˆä¸APIä¸€è‡´ï¼Œä½¿ç”¨MobileBannerCrawlerï¼‰
                try:
                    from services.mobile_banner_crawler import MobileBannerCrawler
                    crawler = MobileBannerCrawler()
                    banner_info_list = crawler.crawl_mobile_banners(download_images=False)
                    logger.info(f"âœ… ä½¿ç”¨ä¼ ç»Ÿçˆ¬è™«æˆåŠŸï¼Œè·å– {len(banner_info_list)} å¼ å›¾ç‰‡")
                except Exception as traditional_error:
                    logger.error(f"âŒ ä¼ ç»Ÿçˆ¬è™«ä¹Ÿå¤±è´¥: {traditional_error}")
                    raise traditional_error
            
            # æ›´æ–°ç¼“å­˜ï¼ˆupdate_cacheæ–¹æ³•ä¼šæ ¹æ®æ•°æ®æƒ…å†µè®¾ç½®æ­£ç¡®çš„çŠ¶æ€ï¼‰
            banner_cache.update_cache(banner_info_list)
            
            if banner_info_list:
                logger.info(f"âœ… {task_name}å®Œæˆï¼Œå…±æ›´æ–° {len(banner_info_list)} å¼ è½®æ’­å›¾ï¼ŒçŠ¶æ€å·²è®¾ä¸ºREADY")
            else:
                logger.warning(f"âš ï¸ {task_name}å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•è½®æ’­å›¾ï¼ŒçŠ¶æ€ä¿æŒPREPARING")
            
        except Exception as e:
            logger.error(f"âŒ {task_name}å¤±è´¥: {e}", exc_info=True)
            # è®¾ç½®é”™è¯¯çŠ¶æ€
            banner_cache = get_banner_cache()
            banner_cache.set_status(ServiceStatus.ERROR, str(e))
            banner_cache.set_updating(False)  # ç¡®ä¿åœæ­¢æ›´æ–°çŠ¶æ€
    
    async def _update_banner_cache_job(self):
        """å®šæ—¶æ›´æ–°è½®æ’­å›¾ç¼“å­˜ä»»åŠ¡"""
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè½®æ’­å›¾çˆ¬è™«ä»»åŠ¡
            task_name = "å®šæ—¶è½®æ’­å›¾æ›´æ–°ä»»åŠ¡"
            future = self.thread_pool.submit(self._run_banner_crawler_in_thread, task_name)
            # ä¸ç­‰å¾…å®Œæˆï¼Œè®©ä»»åŠ¡åœ¨åå°æ‰§è¡Œ
            logger.info(f"{task_name}å·²æäº¤åˆ°åå°çº¿ç¨‹")
            
        except Exception as e:
            logger.error(f"æäº¤å®šæ—¶è½®æ’­å›¾æ›´æ–°ä»»åŠ¡å¤±è´¥: {e}")
    
    async def initial_cache_load(self):
        """åˆå§‹ç¼“å­˜åŠ è½½ï¼ˆæœåŠ¡å¯åŠ¨æ—¶æ‰§è¡Œï¼‰"""
        try:
            logger.info("å¼€å§‹æ‰§è¡Œåˆå§‹ç¼“å­˜åŠ è½½")
            
            # è·å–ç¼“å­˜å®ä¾‹ï¼Œä½†ä¸ç«‹å³è®¾ç½®ä¸ºå‡†å¤‡ä¸­çŠ¶æ€
            # è®©åˆ†æ‰¹å†™å…¥é€»è¾‘æ¥å†³å®šä½•æ—¶è®¾ç½®ä¸ºREADY
            cache = get_news_cache()
            logger.info("ğŸ“¦ åˆ†æ‰¹å†™å…¥æ¨¡å¼ï¼šå°†åœ¨ç¬¬ä¸€æ‰¹æ•°æ®å†™å…¥åç«‹å³å˜ä¸ºå¯ç”¨çŠ¶æ€")
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
            future = self.thread_pool.submit(self._run_crawler_in_thread, "åˆå§‹ç¼“å­˜åŠ è½½", NewsSource.ALL)
            
            # åŒæ—¶å¯åŠ¨è½®æ’­å›¾åˆå§‹åŠ è½½
            banner_future = self.thread_pool.submit(self._run_banner_crawler_in_thread, "åˆå§‹è½®æ’­å›¾åŠ è½½")
            
            # ä¸ç­‰å¾…å®Œæˆï¼Œè®©ä»»åŠ¡åœ¨åå°æ‰§è¡Œï¼ŒæœåŠ¡å¯ä»¥ç«‹å³å¯åŠ¨
            logger.info("åˆå§‹ç¼“å­˜åŠ è½½ä»»åŠ¡å·²æäº¤åˆ°åå°çº¿ç¨‹ï¼ŒæœåŠ¡å¯ä»¥ç«‹å³å“åº”è¯·æ±‚")
            logger.info("åˆå§‹è½®æ’­å›¾åŠ è½½ä»»åŠ¡å·²æäº¤åˆ°åå°çº¿ç¨‹")
            
        except Exception as e:
            logger.error(f"æäº¤åˆå§‹ç¼“å­˜åŠ è½½ä»»åŠ¡å¤±è´¥: {e}")
            # è®¾ç½®é”™è¯¯çŠ¶æ€
            cache = get_news_cache()
            cache.set_status(ServiceStatus.ERROR, str(e))
    
    async def manual_crawl(self, source: NewsSource = NewsSource.ALL):
        """æ‰‹åŠ¨è§¦å‘çˆ¬å–ä»»åŠ¡"""
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œæ‰‹åŠ¨çˆ¬å–ä»»åŠ¡ - æ¥æº: {source.value}")
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
            task_name = f"æ‰‹åŠ¨çˆ¬å–ä»»åŠ¡ - {source.value}"
            future = self.thread_pool.submit(self._run_crawler_in_thread, task_name, source)
            
            # ä¸ç­‰å¾…å®Œæˆï¼Œè®©ä»»åŠ¡åœ¨åå°æ‰§è¡Œ
            logger.info(f"{task_name}å·²æäº¤åˆ°åå°çº¿ç¨‹")
            
        except Exception as e:
            logger.error(f"æäº¤æ‰‹åŠ¨çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}")
            # è®¾ç½®é”™è¯¯çŠ¶æ€
            cache = get_news_cache()
            cache.set_status(ServiceStatus.ERROR, str(e))
    
    async def manual_banner_crawl(self):
        """æ‰‹åŠ¨è§¦å‘è½®æ’­å›¾çˆ¬å–ä»»åŠ¡"""
        try:
            logger.info("å¼€å§‹æ‰§è¡Œæ‰‹åŠ¨è½®æ’­å›¾çˆ¬å–ä»»åŠ¡")
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè½®æ’­å›¾çˆ¬è™«ä»»åŠ¡
            task_name = "æ‰‹åŠ¨è½®æ’­å›¾çˆ¬å–ä»»åŠ¡"
            future = self.thread_pool.submit(self._run_banner_crawler_in_thread, task_name)
            
            # ä¸ç­‰å¾…å®Œæˆï¼Œè®©ä»»åŠ¡åœ¨åå°æ‰§è¡Œ
            logger.info(f"{task_name}å·²æäº¤åˆ°åå°çº¿ç¨‹")
            
        except Exception as e:
            logger.error(f"æäº¤æ‰‹åŠ¨è½®æ’­å›¾çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}")
            # è®¾ç½®é”™è¯¯çŠ¶æ€
            banner_cache = get_banner_cache()
            banner_cache.set_status(ServiceStatus.ERROR, str(e))
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        try:
            self.scheduler.start()
            logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨")
        except Exception as e:
            logger.error(f"å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥: {e}")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        try:
            self.scheduler.shutdown()
            self.thread_pool.shutdown(wait=True)
            logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")
        except Exception as e:
            logger.error(f"åœæ­¢è°ƒåº¦å™¨å¤±è´¥: {e}")
    
    def add_job(self, func, trigger, **kwargs):
        """æ·»åŠ æ–°çš„å®šæ—¶ä»»åŠ¡"""
        try:
            self.scheduler.add_job(func, trigger=trigger, **kwargs)
            logger.info(f"æ·»åŠ å®šæ—¶ä»»åŠ¡æˆåŠŸ: {kwargs.get('name', 'æœªå‘½åä»»åŠ¡')}")
        except Exception as e:
            logger.error(f"æ·»åŠ å®šæ—¶ä»»åŠ¡å¤±è´¥: {e}")
    
    def remove_job(self, job_id):
        """ç§»é™¤å®šæ—¶ä»»åŠ¡"""
        try:
            self.scheduler.remove_job(job_id)
            logger.info(f"ç§»é™¤å®šæ—¶ä»»åŠ¡æˆåŠŸ: {job_id}")
        except Exception as e:
            logger.error(f"ç§»é™¤å®šæ—¶ä»»åŠ¡å¤±è´¥: {e}")
    
    def get_jobs(self):
        """è·å–æ‰€æœ‰å®šæ—¶ä»»åŠ¡"""
        return self.scheduler.get_jobs()

# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
_scheduler: Optional[TaskScheduler] = None

def get_scheduler() -> TaskScheduler:
    """è·å–è°ƒåº¦å™¨å®ä¾‹"""
    global _scheduler
    if _scheduler is None:
        _scheduler = TaskScheduler()
    return _scheduler

def start_scheduler():
    """å¯åŠ¨è°ƒåº¦å™¨"""
    scheduler = get_scheduler()
    scheduler.start()

def stop_scheduler():
    """åœæ­¢è°ƒåº¦å™¨"""
    global _scheduler
    if _scheduler:
        _scheduler.stop()
        _scheduler = None 
