# Copyright (c) 2025 XBXyftx
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import requests
from bs4 import BeautifulSoup
import json
import re
import time
from urllib.parse import urljoin, urlparse
from datetime import datetime
import logging
import os

logger = logging.getLogger(__name__)

class MobileBannerCrawler:
    """
    ä¸“é—¨ç”¨äºçˆ¬å–OpenHarmonyå®˜ç½‘æ‰‹æœºç‰ˆçš„bannerå›¾ç‰‡çˆ¬è™«
    æ¨¡æ‹Ÿæ‰‹æœºæµè§ˆå™¨UAè®¿é—®ï¼Œè·å–æ‰‹æœºç‰ˆé¡µé¢çš„banner-imgç±»åå›¾ç‰‡
    """
    
    def __init__(self):
        self.base_url = "https://old.openharmony.cn"
        self.target_url = "https://old.openharmony.cn/mainPlay"
        self.source = "OpenHarmony-Mobile-Banner"
        self.session = requests.Session()
        
        # æ‰‹æœºç«¯User-Agentæ± 
        self.mobile_user_agents = [
            # iPhone Safari
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
            
            # Android Chrome
            'Mozilla/5.0 (Linux; Android 13; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
            'Mozilla/5.0 (Linux; Android 12; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36',
            'Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Mobile Safari/537.36',
            
            # Huawei/Honoræ‰‹æœº
            'Mozilla/5.0 (Linux; Android 12; NOH-AL00) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
            'Mozilla/5.0 (Linux; Android 11; ELS-AN00) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36',
            
            # å°ç±³æ‰‹æœº
            'Mozilla/5.0 (Linux; Android 13; MI 13) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
            'Mozilla/5.0 (Linux; Android 12; MI 12) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36',
            
            # iPad
            'Mozilla/5.0 (iPad; CPU OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (iPad; CPU OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
        ]
        
        # è®¾ç½®æ‰‹æœºç«¯è¯·æ±‚å¤´
        self.setup_mobile_headers()
    
    def setup_mobile_headers(self):
        """è®¾ç½®æ‰‹æœºç«¯è¯·æ±‚å¤´"""
        import random
        
        mobile_ua = random.choice(self.mobile_user_agents)
        
        self.session.headers.update({
            'User-Agent': mobile_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            # æ¨¡æ‹Ÿæ‰‹æœºç«¯viewport
            'Viewport-Width': '390',
            'Device-Memory': '8',
            'DPR': '3',
        })
        
        logger.info(f"ğŸ“± å·²è®¾ç½®æ‰‹æœºç«¯è¯·æ±‚å¤´ï¼ŒUser-Agent: {mobile_ua[:50]}...")
    
    def get_mobile_page_content(self, url):
        """
        è·å–æ‰‹æœºç‰ˆé¡µé¢å†…å®¹
        """
        try:
            logger.info(f"ğŸ“± æ­£åœ¨è¯·æ±‚æ‰‹æœºç‰ˆé¡µé¢: {url}")
            
            # é‡æ–°è®¾ç½®éšæœºçš„æ‰‹æœºç«¯UAï¼Œé˜²æ­¢è¢«è¯†åˆ«
            self.setup_mobile_headers()
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # æ£€æŸ¥å“åº”æ˜¯å¦æ˜¯æ‰‹æœºç‰ˆ
            content_length = len(response.text)
            logger.info(f"ğŸ“± é¡µé¢åŠ è½½æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
            
            # ç®€å•æ£€æŸ¥æ˜¯å¦è·å–åˆ°äº†ç§»åŠ¨ç‰ˆæœ¬
            if 'viewport' in response.text.lower() or 'mobile' in response.text.lower():
                logger.info("âœ… æˆåŠŸè·å–æ‰‹æœºç‰ˆé¡µé¢å†…å®¹")
            else:
                logger.warning("âš ï¸ å¯èƒ½æœªè·å–åˆ°æ‰‹æœºç‰ˆé¡µé¢ï¼Œä½†ç»§ç»­å¤„ç†")
            
            return response.text
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚æ‰‹æœºç‰ˆé¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ è·å–æ‰‹æœºç‰ˆé¡µé¢å†…å®¹æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return None
    
    def extract_banner_images(self, html_content):
        """
        ä»HTMLå†…å®¹ä¸­æå–bannerç›¸å…³çš„å›¾ç‰‡
        åŒ…æ‹¬banner-imgç±»åã€è½®æ’­å›¾ã€ä¸»è¦å±•ç¤ºå›¾ç‰‡ç­‰
        """
        banner_images = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            logger.info("ğŸ” å¼€å§‹è§£æHTMLå†…å®¹ï¼ŒæŸ¥æ‰¾bannerç›¸å…³å›¾ç‰‡...")
            
            # 1. æŸ¥æ‰¾æ‰€æœ‰åŒ…å«banner-imgç±»åçš„å…ƒç´ 
            banner_img_elements = soup.find_all(class_=re.compile(r'.*banner-img.*'))
            logger.info(f"ğŸ” æ‰¾åˆ° {len(banner_img_elements)} ä¸ªåŒ…å« banner-img ç±»åçš„å…ƒç´ ")
            
            # 2. æŸ¥æ‰¾å…¶ä»–bannerç›¸å…³çš„ç±»å
            banner_class_patterns = [
                r'.*banner.*',
                r'.*swiper.*slide.*',
                r'.*carousel.*',
                r'.*slider.*',
                r'.*hero.*',
                r'.*main.*banner.*',
                r'.*top.*banner.*'
            ]
            
            for pattern in banner_class_patterns:
                elements = soup.find_all(class_=re.compile(pattern, re.IGNORECASE))
                banner_img_elements.extend(elements)
                logger.info(f"ğŸ” é€šè¿‡æ¨¡å¼ '{pattern}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
            
            # 3. å¤„ç†æ‰¾åˆ°çš„bannerå…ƒç´ 
            for idx, element in enumerate(banner_img_elements):
                logger.debug(f"å¤„ç†ç¬¬ {idx + 1} ä¸ªbannerå…ƒç´ : {element.name}, class: {element.get('class')}")
                
                # å¤„ç†imgæ ‡ç­¾
                if element.name == 'img':
                    img_url = self.extract_image_url(element)
                    if img_url:
                        image_info = self.create_image_info(img_url, element, f"banner-img-{idx + 1}")
                        banner_images.append(image_info)
                        logger.info(f"âœ… æå–åˆ°bannerå›¾ç‰‡: {img_url}")
                
                # å¤„ç†åŒ…å«imgå­å…ƒç´ çš„å®¹å™¨
                img_tags = element.find_all('img')
                for img_idx, img_tag in enumerate(img_tags):
                    img_url = self.extract_image_url(img_tag)
                    if img_url:
                        image_info = self.create_image_info(img_url, img_tag, f"banner-container-{idx + 1}-{img_idx + 1}")
                        banner_images.append(image_info)
                        logger.info(f"âœ… æå–åˆ°åµŒå¥—bannerå›¾ç‰‡: {img_url}")
                
                # å¤„ç†CSSèƒŒæ™¯å›¾ç‰‡
                bg_url = self.extract_background_image(element)
                if bg_url:
                    image_info = self.create_image_info(bg_url, element, f"banner-bg-{idx + 1}")
                    banner_images.append(image_info)
                    logger.info(f"âœ… æå–åˆ°bannerèƒŒæ™¯å›¾ç‰‡: {bg_url}")
            
            # 4. æŸ¥æ‰¾é¡µé¢ä¸­æ‰€æœ‰è¾ƒå¤§çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯bannerï¼‰
            all_images = soup.find_all('img')
            logger.info(f"ğŸ” é¡µé¢æ€»å…±æœ‰ {len(all_images)} å¼ å›¾ç‰‡ï¼Œç­›é€‰å¯èƒ½çš„bannerå›¾ç‰‡...")
            
            for idx, img in enumerate(all_images):
                img_url = self.extract_image_url(img)
                if img_url:
                    # æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦åŒ…å«bannerç›¸å…³å…³é”®è¯
                    banner_keywords = ['banner', 'slide', 'carousel', 'hero', 'main', 'top', 'header']
                    img_url_lower = img_url.lower()
                    
                    if any(keyword in img_url_lower for keyword in banner_keywords):
                        image_info = self.create_image_info(img_url, img, f"potential-banner-{idx + 1}")
                        banner_images.append(image_info)
                        logger.info(f"âœ… å‘ç°å¯èƒ½çš„bannerå›¾ç‰‡ï¼ˆé€šè¿‡URLå…³é”®è¯ï¼‰: {img_url}")
                    
                    # æ£€æŸ¥imgæ ‡ç­¾çš„classæˆ–å…¶ä»–å±æ€§
                    class_list = img.get('class', [])
                    class_str = ' '.join(class_list) if isinstance(class_list, list) else str(class_list)
                    
                    if any(keyword in class_str.lower() for keyword in banner_keywords):
                        image_info = self.create_image_info(img_url, img, f"class-banner-{idx + 1}")
                        banner_images.append(image_info)
                        logger.info(f"âœ… å‘ç°å¯èƒ½çš„bannerå›¾ç‰‡ï¼ˆé€šè¿‡classï¼‰: {img_url}")
            
            # 5. é¢å¤–æŸ¥æ‰¾ï¼šé€šè¿‡dataå±æ€§æŸ¥æ‰¾å¯èƒ½çš„bannerå›¾ç‰‡
            data_banner_elements = soup.find_all(attrs={"data-banner": True}) + soup.find_all(attrs={"data-bg": True})
            for idx, element in enumerate(data_banner_elements):
                for attr in ['data-banner', 'data-bg', 'data-src', 'data-original']:
                    img_url = element.get(attr)
                    if img_url and self.is_valid_image_url(img_url):
                        img_url = urljoin(self.base_url, img_url)
                        image_info = self.create_image_info(img_url, element, f"data-banner-{idx + 1}")
                        banner_images.append(image_info)
                        logger.info(f"âœ… é€šè¿‡dataå±æ€§æå–åˆ°å›¾ç‰‡: {img_url}")
                        break
            
            # 6. å»é‡
            unique_images = []
            seen_urls = set()
            for img in banner_images:
                if img['url'] not in seen_urls:
                    unique_images.append(img)
                    seen_urls.add(img['url'])
            
            logger.info(f"ğŸ¯ å…±æå–åˆ° {len(unique_images)} å¼ å”¯ä¸€çš„bannerç›¸å…³å›¾ç‰‡")
            
            # 7. å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè®°å½•é¡µé¢ç»“æ„ç”¨äºè°ƒè¯•
            if len(unique_images) == 0:
                logger.warning("ğŸ” æœªæ‰¾åˆ°bannerå›¾ç‰‡ï¼Œåˆ†æé¡µé¢ç»“æ„...")
                self._debug_page_structure(soup)
            
            return unique_images
            
        except Exception as e:
            logger.error(f"âŒ è§£æbannerå›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []
    
    def _debug_page_structure(self, soup):
        """
        è°ƒè¯•é¡µé¢ç»“æ„ï¼Œè¾“å‡ºå…³é”®ä¿¡æ¯
        """
        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«å›¾ç‰‡çš„å…ƒç´ 
            img_count = len(soup.find_all('img'))
            logger.info(f"ğŸ“Š é¡µé¢è°ƒè¯•ä¿¡æ¯ï¼š")
            logger.info(f"   - æ€»å›¾ç‰‡æ•°é‡: {img_count}")
            
            # è¾“å‡ºå‰5ä¸ªå›¾ç‰‡çš„ä¿¡æ¯
            images = soup.find_all('img')[:5]
            for i, img in enumerate(images):
                src = img.get('src', img.get('data-src', 'æ— '))
                class_name = img.get('class', 'æ— ')
                alt = img.get('alt', 'æ— ')
                logger.info(f"   - å›¾ç‰‡{i+1}: src={src[:50]}..., class={class_name}, alt={alt}")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è½®æ’­æˆ–bannerå®¹å™¨
            possible_containers = soup.find_all(['div', 'section'], class_=re.compile(r'.*(banner|swiper|carousel|slider|hero).*', re.IGNORECASE))
            logger.info(f"   - å¯èƒ½çš„bannerå®¹å™¨æ•°é‡: {len(possible_containers)}")
            
            for i, container in enumerate(possible_containers[:3]):
                class_name = container.get('class', 'æ— ')
                child_imgs = len(container.find_all('img'))
                logger.info(f"   - å®¹å™¨{i+1}: class={class_name}, åŒ…å«å›¾ç‰‡={child_imgs}å¼ ")
                
        except Exception as e:
            logger.error(f"è°ƒè¯•é¡µé¢ç»“æ„æ—¶å‡ºé”™: {e}")
    
    def extract_image_url(self, img_element):
        """
        ä»imgå…ƒç´ ä¸­æå–å›¾ç‰‡URL
        """
        # å°è¯•å¤šç§å¯èƒ½çš„å›¾ç‰‡URLå±æ€§
        url_attributes = ['src', 'data-src', 'data-original', 'data-lazy', 'data-echo', 'srcset']
        
        for attr in url_attributes:
            img_url = img_element.get(attr)
            if img_url:
                # å¤„ç†srcsetå±æ€§ï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªURLï¼‰
                if attr == 'srcset':
                    # srcsetæ ¼å¼: "url1 1x, url2 2x" æˆ– "url1 480w, url2 800w"
                    urls = re.findall(r'(https?://[^\s,]+)', img_url)
                    if urls:
                        img_url = urls[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªURL
                    else:
                        continue
                
                if self.is_valid_image_url(img_url):
                    return urljoin(self.base_url, img_url)
        
        return None
    
    def extract_background_image(self, element):
        """
        ä»å…ƒç´ çš„styleå±æ€§ä¸­æå–èƒŒæ™¯å›¾ç‰‡URL
        """
        style = element.get('style', '')
        if 'background-image' in style:
            # åŒ¹é… background-image: url('...') æˆ– background-image: url("...")
            bg_match = re.search(r'background-image:\s*url\([\'"]?([^\'"()]+)[\'"]?\)', style)
            if bg_match:
                bg_url = bg_match.group(1)
                if self.is_valid_image_url(bg_url):
                    return urljoin(self.base_url, bg_url)
        
        return None
    
    def is_valid_image_url(self, url):
        """
        éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡URL
        """
        if not url or len(url.strip()) == 0:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æ‰©å±•å
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg']
        url_lower = url.lower()
        
        # ç›´æ¥æ£€æŸ¥æ‰©å±•å
        for ext in image_extensions:
            if ext in url_lower:
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ç›¸å…³è·¯å¾„
        image_keywords = ['image', 'img', 'pic', 'photo', 'banner', 'bg']
        for keyword in image_keywords:
            if keyword in url_lower:
                return True
        
        # å¦‚æœURLçœ‹èµ·æ¥åƒæ˜¯base64æ•°æ®ï¼Œè·³è¿‡
        if url.startswith('data:'):
            return True if 'image' in url else False
        
        return False
    
    def create_image_info(self, img_url, element, image_id):
        """
        åˆ›å»ºå›¾ç‰‡ä¿¡æ¯å¯¹è±¡
        """
        # è·å–å›¾ç‰‡çš„altå±æ€§æˆ–å…¶ä»–æè¿°ä¿¡æ¯
        alt_text = element.get('alt', '')
        title_text = element.get('title', '')
        class_names = element.get('class', [])
        
        # å°è¯•ä»URLä¸­æå–æ–‡ä»¶å
        parsed_url = urlparse(img_url)
        filename = os.path.basename(parsed_url.path) or f"banner_image_{image_id}"
        
        return {
            "id": image_id,
            "url": img_url,
            "alt": alt_text,
            "title": title_text,
            "filename": filename,
            "classes": class_names if isinstance(class_names, list) else [class_names] if class_names else [],
            "source": self.source,
            "extracted_at": datetime.now().isoformat(),
            "page_url": self.target_url
        }
    
    def download_image(self, image_info, save_directory="downloads/banners"):
        """
        ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
        """
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            os.makedirs(save_directory, exist_ok=True)
            
            img_url = image_info['url']
            filename = image_info['filename']
            
            # ç¡®ä¿æ–‡ä»¶åæœ‰æ‰©å±•å
            if not any(filename.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']):
                # ä»URLä¸­å°è¯•è·å–æ‰©å±•å
                url_ext = os.path.splitext(urlparse(img_url).path)[1]
                if url_ext:
                    filename += url_ext
                else:
                    filename += '.jpg'  # é»˜è®¤æ‰©å±•å
            
            file_path = os.path.join(save_directory, filename)
            
            logger.info(f"â¬‡ï¸ å¼€å§‹ä¸‹è½½å›¾ç‰‡: {img_url}")
            
            response = self.session.get(img_url, timeout=30)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            file_size = len(response.content)
            logger.info(f"âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {file_path} (å¤§å°: {file_size} bytes)")
            
            # æ›´æ–°å›¾ç‰‡ä¿¡æ¯
            image_info['local_path'] = file_path
            image_info['file_size'] = file_size
            image_info['downloaded'] = True
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_url}, é”™è¯¯: {e}")
            image_info['downloaded'] = False
            image_info['download_error'] = str(e)
            return False
    
    def crawl_mobile_banners(self, download_images=True, save_directory="downloads/banners"):
        """
        ä¸»è¦æ–¹æ³•ï¼šçˆ¬å–æ‰‹æœºç‰ˆbannerå›¾ç‰‡
        
        Args:
            download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
            save_directory: å›¾ç‰‡ä¿å­˜ç›®å½•
            
        Returns:
            åŒ…å«æ‰€æœ‰bannerå›¾ç‰‡ä¿¡æ¯çš„åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–OpenHarmonyæ‰‹æœºç‰ˆbannerå›¾ç‰‡")
        logger.info(f"ğŸ¯ ç›®æ ‡URL: {self.target_url}")
        
        # è·å–æ‰‹æœºç‰ˆé¡µé¢å†…å®¹
        html_content = self.get_mobile_page_content(self.target_url)
        if not html_content:
            logger.error("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹ï¼Œçˆ¬å–å¤±è´¥")
            return []
        
        # æå–bannerå›¾ç‰‡
        banner_images = self.extract_banner_images(html_content)
        
        if not banner_images:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•bannerå›¾ç‰‡")
            return []
        
        logger.info(f"ğŸ‰ æˆåŠŸæå–åˆ° {len(banner_images)} å¼ bannerå›¾ç‰‡")
        
        # ä¸‹è½½å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if download_images:
            logger.info(f"â¬‡ï¸ å¼€å§‹ä¸‹è½½å›¾ç‰‡åˆ°ç›®å½•: {save_directory}")
            download_success_count = 0
            
            for img_info in banner_images:
                if self.download_image(img_info, save_directory):
                    download_success_count += 1
                time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            logger.info(f"ğŸ“ å›¾ç‰‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸä¸‹è½½ {download_success_count}/{len(banner_images)} å¼ å›¾ç‰‡")
        
        # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        result_file = os.path.join(save_directory, "banner_images_info.json")
        try:
            os.makedirs(os.path.dirname(result_file), exist_ok=True)
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "crawl_time": datetime.now().isoformat(),
                    "target_url": self.target_url,
                    "total_images": len(banner_images),
                    "images": banner_images
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ å›¾ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ°: {result_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å›¾ç‰‡ä¿¡æ¯å¤±è´¥: {e}")
        
        return banner_images
    
    def get_banner_summary(self, banner_images):
        """
        è·å–bannerå›¾ç‰‡çš„ç»Ÿè®¡æ‘˜è¦
        """
        if not banner_images:
            return {"total": 0, "downloaded": 0, "failed": 0, "success_rate": "0%"}
        
        total = len(banner_images)
        downloaded = sum(1 for img in banner_images if img.get('downloaded', False))
        failed = total - downloaded
        
        success_rate = f"{(downloaded / total * 100):.1f}%" if total > 0 else "0%"
        return {
            "total": total,
            "downloaded": downloaded,
            "failed": failed,
            "success_rate": success_rate
        }

def main():
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨MobileBannerCrawler
    """
    print("ğŸ“± OpenHarmonyæ‰‹æœºç‰ˆBannerå›¾ç‰‡çˆ¬è™«å¯åŠ¨...")
    print("=" * 60)
    
    crawler = MobileBannerCrawler()
    
    try:
        # çˆ¬å–bannerå›¾ç‰‡
        banner_images = crawler.crawl_mobile_banners(
            download_images=True,
            save_directory="downloads/mobile_banners"
        )
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        summary = crawler.get_banner_summary(banner_images)
        print(f"\nğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡:")
        print(f"   æ€»è®¡å›¾ç‰‡: {summary['total']} å¼ ")
        print(f"   ä¸‹è½½æˆåŠŸ: {summary['downloaded']} å¼ ")
        print(f"   ä¸‹è½½å¤±è´¥: {summary['failed']} å¼ ")
        print(f"   æˆåŠŸç‡: {summary['success_rate']}")
        
        # æ˜¾ç¤ºå›¾ç‰‡åˆ—è¡¨
        if banner_images:
            print(f"\nğŸ“‹ Bannerå›¾ç‰‡åˆ—è¡¨:")
            for i, img in enumerate(banner_images, 1):
                print(f"   {i}. {img['filename']}")
                print(f"      URL: {img['url']}")
                print(f"      Alt: {img['alt'] or 'æ— '}")
                print(f"      ä¸‹è½½çŠ¶æ€: {'âœ… æˆåŠŸ' if img.get('downloaded') else 'âŒ å¤±è´¥'}")
                print()
        
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    main()
