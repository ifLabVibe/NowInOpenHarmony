# Copyright (c) 2025 XBXyftx
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import requests
from bs4 import BeautifulSoup
import json
import re
import time
from urllib.parse import urljoin
from datetime import datetime

class OpenHarmonyNewsCrawler:
    def __init__(self):
        self.base_url = "https://old.openharmony.cn"
        self.source = "OpenHarmony"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def get_page_content(self, url):
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None

    def get_all_article_infos(self):
        """åˆ†é¡µéå†APIï¼Œè·å–æ‰€æœ‰æ–°é—»çš„urlã€titleã€dateï¼Œå»é‡å¹¶æ ¡éªŒæœ‰æ•ˆæ€§"""
        all_infos = {}
        page_num = 1
        page_size = 300  # è®¾ç½®ä¸º300ï¼Œä¸€æ¬¡æ€§è·å–æ›´å¤šæ•°æ®ï¼Œå‡å°‘APIè¯·æ±‚æ¬¡æ•°

        print(f"ğŸš€ å¼€å§‹é«˜æ•ˆè·å–OpenHarmonyæ–‡ç« ä¿¡æ¯ï¼Œæ¯é¡µ{page_size}æ¡æ•°æ®...")

        while True:
            api_url = f"{self.base_url}/backend/knowledge/secondaryPage/queryBatch?type=3&pageNum={page_num}&pageSize={page_size}"
            print(f"ğŸ“¡ è¯·æ±‚API: ç¬¬{page_num}é¡µ")
            try:
                resp = self.session.get(api_url, timeout=15)  # å¢åŠ è¶…æ—¶æ—¶é—´
                resp.raise_for_status()
                response_data = resp.json()
                data = response_data.get("data", [])

                # æ‰“å°å“åº”ä¿¡æ¯ç”¨äºè°ƒè¯•
                print(f"ğŸ“Š ç¬¬{page_num}é¡µè·å–åˆ°{len(data)}æ¡æ•°æ®")

            except Exception as e:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
                break

            if not data:
                print(f"âœ… ç¬¬{page_num}é¡µæ— æ•°æ®ï¼Œçˆ¬å–å®Œæˆ")
                break

            # å¤„ç†æœ¬é¡µæ•°æ®
            page_count = 0
            for item in data:
                url = item.get("url")
                title = item.get("title", "")
                date = item.get("startTime", "")

                # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
                standardized_date = self._standardize_date(date)

                if url and url not in all_infos:
                    all_infos[url] = {"title": title, "date": standardized_date}
                    page_count += 1

            print(f"ğŸ“ˆ ç¬¬{page_num}é¡µæ–°å¢{page_count}æ¡æœ‰æ•ˆæ•°æ®ï¼Œç´¯è®¡{len(all_infos)}æ¡")

            # å¦‚æœæœ¬é¡µæ•°æ®é‡å°äºpage_sizeï¼Œè¯´æ˜å·²ç»è·å–å®Œæ‰€æœ‰æ•°æ®
            if len(data) < page_size:
                print(f"ğŸ¯ ç¬¬{page_num}é¡µæ•°æ®é‡({len(data)})å°äºé¡µé¢å¤§å°({page_size})ï¼Œçˆ¬å–å®Œæˆ")
                break

            page_num += 1
            time.sleep(0.3)  # å‡å°‘ç­‰å¾…æ—¶é—´ï¼Œä»0.5ç§’é™åˆ°0.3ç§’

        print(f"ğŸ“‹ å…±ï¿½ï¿½ï¿½å–åˆ°{len(all_infos)}æ¡æœ‰æ•ˆæ–‡ç« ä¿¡æ¯")

        # å¿«é€Ÿæœ‰æ•ˆæ€§æ ¡éªŒï¼ˆåªæ£€æŸ¥å‰10ä¸ªURLï¼Œå¦‚æœå¤§éƒ¨åˆ†æœ‰æ•ˆå°±è®¤ä¸ºå…¨éƒ¨æœ‰æ•ˆï¼‰
        print("ğŸ” è¿›è¡Œå¿«é€Ÿæœ‰æ•ˆæ€§æ ¡éªŒ...")
        test_urls = list(all_infos.keys())[:min(10, len(all_infos))]
        valid_test_count = 0

        for url in test_urls:
            try:
                r = self.session.head(url, timeout=3)  # å‡å°‘è¶…æ—¶æ—¶é—´
                if r.status_code == 200:
                    valid_test_count += 1
            except:
                continue

        validity_rate = valid_test_count / len(test_urls) if test_urls else 0
        print(f"âœ… å¿«é€Ÿæ ¡éªŒå®Œæˆï¼š{valid_test_count}/{len(test_urls)} æœ‰æ•ˆï¼Œæœ‰æ•ˆç‡{validity_rate:.1%}")

        # å¦‚æœæœ‰æ•ˆç‡é«˜ï¼Œç›´æ¥è¿”å›æ‰€æœ‰æ•°æ®ï¼Œå¦åˆ™è¿›è¡Œå®Œæ•´æ ¡éªŒ
        if validity_rate >= 0.8:  # 80%ä»¥ä¸Šæœ‰æ•ˆå°±ç›´æ¥ä½¿ç”¨
            print("ğŸš€ æœ‰æ•ˆç‡é«˜ï¼Œè·³è¿‡å®Œæ•´æ ¡éªŒï¼Œç›´æ¥è¿”å›æ‰€æœ‰æ•°æ®")
            return [{"url": url, "title": info["title"], "date": info["date"]}
                    for url, info in all_infos.items()]
        else:
            print("ğŸŒ æœ‰æ•ˆç‡è¾ƒä½ï¼Œè¿›è¡Œå®Œæ•´URLæœ‰æ•ˆæ€§æ ¡éªŒ...")
            valid_infos = []
            for url, info in all_infos.items():
                try:
                    r = self.session.head(url, timeout=5)
                    if r.status_code == 200:
                        valid_infos.append({"url": url, "title": info["title"], "date": info["date"]})
                except:
                    continue
            print(f"âœ… å®Œæ•´æ ¡éªŒå®Œæˆï¼Œæœ‰æ•ˆURLæ•°é‡: {len(valid_infos)}")
            return valid_infos

    def parse_article_content(self, article_url):
        content = self.get_page_content(article_url)
        if not content:
            return []
        soup = BeautifulSoup(content, 'html.parser')
        result_data = []
        article_container = (
            soup.find(id='js_content') or
            soup.find(class_='rich_media_content') or
            soup.find(id='page-content') or
            soup.find(class_='rich_media_area_primary') or
            soup.find(class_=re.compile(r'article|content|detail', re.I)) or
            soup.find('article') or
            soup.find(id=re.compile(r'article|content|detail', re.I))
        )
        if not article_container:
            article_container = soup.find('body')
        if article_container:
            for element in article_container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'img', 'video']):
                if element.name in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div']:
                    text = element.get_text().strip()
                    if text and len(text) > 10:
                        result_data.append({"type": "text", "value": text})
                elif element.name == 'img':
                    img_src = element.get('data-src') or element.get('data-original') or element.get('src')
                    if img_src:
                        img_url = urljoin(self.base_url, img_src)
                        result_data.append({"type": "image", "value": img_url})
                elif element.name == 'video':
                    video_src = element.get('src')
                    if video_src:
                        video_url = urljoin(self.base_url, video_src)
                        result_data.append({"type": "video", "value": video_url})
                    for source in element.find_all('source'):
                        video_src = source.get('src')
                        if video_src:
                            video_url = urljoin(self.base_url, video_src)
                            result_data.append({"type": "video", "value": video_url})
        return result_data

    def _standardize_date(self, date_str):
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ï¼Œå°†å¤šç§æ—¥æœŸæ ¼å¼ç»Ÿä¸€ä¸ºYYYY-MM-DDæ ¼å¼"""
        if not date_str:
            return ''

        try:
            # åŒ¹é…å¤šç§æ—¥æœŸæ ¼å¼ï¼š2025.9.13, 2025-9-13, 2025/9/13, 2025å¹´9æœˆ13æ—¥
            date_pattern = r'(\d{4})[.\-\/å¹´](\d{1,2})[.\-\/æœˆ](\d{1,2})[æ—¥]?'
            match = re.search(date_pattern, str(date_str))

            if match:
                year, month, day = match.groups()
                # æ ¼å¼åŒ–ä¸ºç»Ÿä¸€çš„YYYY-MM-DDæ ¼å¼
                return f"{year}-{int(month):02d}-{int(day):02d}"
            else:
                # å°è¯•ç›´æ¥è§£ææ—¥æœŸå­—ç¬¦ä¸²
                # å¤„ç†åªæœ‰å¹´æœˆçš„æƒ…å†µ
                month_pattern = r'(\d{4})[.\-\/å¹´](\d{1,2})[æœˆ]?'
                month_match = re.search(month_pattern, str(date_str))
                if month_match:
                    year, month = month_match.groups()
                    return f"{year}-{int(month):02d}-01"  # é»˜è®¤è®¾ç½®ä¸ºå½“æœˆç¬¬ä¸€å¤©

                # å¦‚æœéƒ½æ— æ³•åŒ¹é…ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²å¹¶è®°å½•æ—¥å¿—
                print(f"âš ï¸ æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}ï¼Œä¿æŒåŸæ ·")
                return date_str
        except Exception as e:
            print(f"âŒ æ—¥æœŸæ ‡å‡†åŒ–å¤±è´¥: {date_str}, é”™è¯¯: {e}")
            return date_str

    def _format_article(self, article):
        """å°†æ–‡ç« æ ¼å¼åŒ–ä¸ºç»Ÿä¸€çš„æ–°é—»æ ¼å¼"""
        import hashlib

        # ç”Ÿæˆæ–‡ç« IDï¼ˆåŸºäºURLçš„å“ˆå¸Œï¼‰
        article_id = hashlib.md5(article['url'].encode()).hexdigest()[:16]

        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        standardized_date = self._standardize_date(article.get('date', ''))

        # è¿”å›ç»Ÿä¸€æ ¼å¼ï¼Œç¬¦åˆTypeScriptæ¥å£è§„èŒƒ
        return {
            "id": article_id,
            "title": article['title'],
            "date": standardized_date,  # ä½¿ç”¨æ ‡å‡†åŒ–åçš„æ—¥æœŸ
            "url": article['url'],
            "content": article.get('content', []),
            "category": "å®˜æ–¹åŠ¨æ€",
            "summary": article.get('summary', ''),
            "source": "OpenHarmony",  # æ˜ç¡®æ ‡æ³¨æ¥æºä¸ºOpenHarmonyå®˜ç½‘
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }

    def crawl_openharmony_news(self, batch_callback=None, batch_size=20):
        import logging
        logger = logging.getLogger(__name__)

        logger.info("ğŸŒ å¼€å§‹çˆ¬å–OpenHarmonyå®˜ç½‘æ–°é—»...")
        if batch_callback:
            logger.info(f"ğŸ“¦ å¯ç”¨åˆ†æ‰¹å¤„ç†æ¨¡å¼ï¼Œæ¯ {batch_size} ç¯‡æ–‡ç« æ‰§è¡Œä¸€æ¬¡å›è°ƒ")

        articles_info = self.get_all_article_infos()
        logger.info(f"ğŸ“‹ è·å–åˆ° {len(articles_info)} ç¯‡æ–‡ç« ä¿¡æ¯")

        all_articles_data = []
        batch_articles = []

        for i, info in enumerate(articles_info):
            title = info["title"]
            date = info["date"]
            article_url = info["url"]
            logger.info(f"ğŸ” æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(articles_info)} ç¯‡æ–‡ç« : {title}")
            logger.debug(f"ğŸ”— æ–‡ç« URL: {article_url}")

            article_data = self.parse_article_content(article_url)
            if article_data:
                article_info = self._format_article({
                    "title": title,
                    "date": date,
                    "url": article_url,
                    "content": article_data
                })
                all_articles_data.append(article_info)
                batch_articles.append(article_info)
                logger.info(f"âœ… æˆåŠŸè§£ææ–‡ç« ï¼Œå…± {len(article_data)} ä¸ªå†…å®¹å—")

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹å¤„ç†å¤§å°
                if len(batch_articles) >= batch_size and batch_callback:
                    try:
                        logger.info(f"ğŸ“¦ [åˆ†æ‰¹å¤„ç†] è¾¾åˆ°æ‰¹å¤„ç†å¤§å° {batch_size}ï¼Œæ‰§è¡Œå›è°ƒ...")
                        batch_callback(batch_articles.copy())
                        batch_articles.clear()  # æ¸…ç©ºå½“å‰æ‰¹æ¬¡
                        logger.info(f"âœ… [åˆ†æ‰¹å¤„ç†] å›è°ƒæ‰§è¡ŒæˆåŠŸï¼Œç»§ç»­å¤„ç†åç»­æ–‡ç« ")
                    except Exception as callback_e:
                        logger.error(f"âŒ [åˆ†æ‰¹å¤„ç†] å›è°ƒæ‰§è¡Œå¤±è´¥: {callback_e}")
            else:
                logger.warning(f"âš ï¸ æ–‡ç« å†…å®¹è§£æå¤±è´¥: {title}")
            time.sleep(1)

        # å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡
        if batch_articles and batch_callback:
            try:
                logger.info(f"ğŸ“¦ [åˆ†æ‰¹å¤„ç†] å¤„ç†æœ€åå‰©ä½™çš„ {len(batch_articles)} ç¯‡æ–‡ç« ...")
                batch_callback(batch_articles.copy())
                logger.info(f"âœ… [åˆ†æ‰¹å¤„ç†] æœ€åæ‰¹æ¬¡å¤„ç†å®Œæˆ")
            except Exception as callback_e:
                logger.error(f"âŒ [åˆ†æ‰¹å¤„ç†] æœ€åæ‰¹æ¬¡å¤„ç†å¤±è´¥: {callback_e}")

        logger.info(f"ğŸ‰ OpenHarmonyå®˜ç½‘çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {len(all_articles_data)} ç¯‡æ–‡ç« ")
        return all_articles_data

def main():
    print("OpenHarmonyå®˜ç½‘æ–°é—»çˆ¬è™«å¯åŠ¨...")
    print("æ³¨æ„ï¼šæ­¤è„šæœ¬éœ€è¦å®‰è£…ä»¥ä¸‹ä¾èµ–:")
    print("  pip install requests beautifulsoup4")
    print("-" * 50)
    crawler = OpenHarmonyNewsCrawler()
    try:
        results = crawler.crawl_openharmony_news()
        if results:
            print(f"\nçˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {len(results)} ç¯‡æ–‡ç« ")
        else:
            print("\nçˆ¬å–å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ")
    except Exception as e:
        print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
